{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dn7q5i03MYxM"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# import numpy\n",
        "\n",
        "# %% [markdown]\n",
        "#     Environment setup\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.nS = 16\n",
        "        self.nA = 4\n",
        "        self.states = [i for i in range(16)]\n",
        "        self.actions = {0: 'Left', 1: 'Up', 2: 'Right', 3: 'Down'}\n",
        "        self.env = {\n",
        "            state: {\n",
        "                action: {\n",
        "                    's_Prob': 0, 'n_State': 0, 's_Reward': 0, 'Terminated': False\n",
        "                } for action in [i for i in range(4)]\n",
        "            } for state in [i for i in range(16)]\n",
        "        }\n",
        "        states_set = set(range(16))\n",
        "        action = 0  # action Left\n",
        "        for state in [0, 4, 8, 12]:\n",
        "            self.env[state][0] = {\n",
        "                's_Prob': 1, 'n_State': state, 's_Reward': -5, 'Terminated': False}\n",
        "\n",
        "        for state in states_set - set([0, 4, 8, 12]):\n",
        "            self.env[state][0] = {\n",
        "                's_Prob': 1, 'n_State': state-1, 's_Reward': -1, 'Terminated': False}\n",
        "\n",
        "        action = 1  # Action Up\n",
        "        for state in [0, 1, 2, 3]:\n",
        "            self.env[state][1] = {\n",
        "                's_Prob': 1, 'n_State': state, 's_Reward': -5, 'Terminated': False}\n",
        "\n",
        "        for state in states_set - set([0, 1, 2, 3]):\n",
        "            self.env[state][1] = {\n",
        "                's_Prob': 1, 'n_State': state-4, 's_Reward': -1, 'Terminated': False}\n",
        "\n",
        "        action = 2  # right\n",
        "        for state in [3, 7, 11]:\n",
        "            self.env[state][2] = {\n",
        "                's_Prob': 1, 'n_State': state, 's_Reward': -5, 'Terminated': False}\n",
        "\n",
        "        for state in states_set - set([3, 7, 11, 15]):\n",
        "            self.env[state][2] = {\n",
        "                's_Prob': 1, 'n_State': state+1, 's_Reward': -1, 'Terminated': False}\n",
        "\n",
        "        action = 3  # action Down\n",
        "        for state in [12, 13, 14]:\n",
        "            self.env[state][3] = {\n",
        "                's_Prob': 1, 'n_State': state, 's_Reward': -5, 'Terminated': False}\n",
        "\n",
        "        for state in states_set - set([12, 13, 14, 15]):\n",
        "            self.env[state][3] = {\n",
        "                's_Prob': 1, 'n_State': state+4, 's_Reward': -1, 'Terminated': False}\n",
        "\n",
        "        state = 15\n",
        "        self.env[state][0] = {'s_Prob': 1, 'n_State': state,\n",
        "                              's_Reward': 0, 'Terminated': True}  # left\n",
        "        self.env[state][1] = {'s_Prob': 1, 'n_State': state,\n",
        "                              's_Reward': 0, 'Terminated': True}  # up\n",
        "        self.env[state][2] = {'s_Prob': 1, 'n_State': state,\n",
        "                              's_Reward': 0, 'Terminated': True}  # right\n",
        "        self.env[state][3] = {'s_Prob': 1, 'n_State': state,\n",
        "                              's_Reward': 0, 'Terminated': True}  # down\n",
        "\n",
        "\n",
        "# %%\n",
        "def reward_funtion(prob_matrix, envirnoment):\n",
        "    state_reward = [[envirnoment.env[state][action]['s_Reward']\n",
        "                     for action in range(4)] for state in range(16)]\n",
        "    # print(state_reward)\n",
        "    reward = state_reward * prob_matrix\n",
        "    # print(reward)\n",
        "    return reward\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "def value_funtion(environment, values, reward, prob_matrix, gamma):   \n",
        "    values = reward + gamma * (prob_matrix * reward)\n",
        "    return values\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "#     main funtion\n",
        "\n",
        "# # %%\n",
        "# def main():\n",
        "#     environment = Environment()\n",
        "#     print(\"Environment --------\")\n",
        "#     print(f'\\nnumber of states : {environment.nS}')\n",
        "#     print(f'number of action at each state : {environment.nA}')\n",
        "#     print(f'states {environment.states}')\n",
        "#     print(f'action {environment.actions.values()}')\n",
        "#     print('\\n\\n')\n",
        "#     # print(f'env {environment.env}')\n",
        "#     prob_matrix_states = [[environment.env[state][action]['s_Prob']\n",
        "#                            for action in range(4)] for state in range(16)]\n",
        "#     # print(prob_matrix)\n",
        "\n",
        "#     # random uniform policy in staring\n",
        "#     action_prob = numpy.ones((environment.nS, environment.nA)) / environment.nA\n",
        "#     print(f\"policy : \\n {action_prob}\")\n",
        "#     # prob matrix to reach state {s'} from {s} given action 'a'\n",
        "#     prob_matrix = prob_matrix_states * action_prob\n",
        "#     # print(prob_matrix)\n",
        "#     # print(prob_matrix.shape)\n",
        "#     reward = reward_funtion(prob_matrix=prob_matrix, envirnoment=environment)\n",
        "#     print(f\"reward : \\n {reward}\")\n",
        "\n",
        "#     values = numpy.zeros((environment.nS, environment.nA))\n",
        "\n",
        "#     values = value_funtion(environment=environment, values=values,\n",
        "#                            reward=reward, prob_matrix=prob_matrix, gamma=1)\n",
        "\n",
        "#     print(f\"values : \\n {values}\")\n",
        "\n",
        "\n",
        "# # %%\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n",
        "\n",
        "# # %%\n",
        "# environment = Environment()\n",
        "# print(environment.env)\n",
        "\n",
        "# # %%\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import numpy as np\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "def one_step_lookahead(environment, state, V, discount_factor):\n",
        "    # Create a vector of dimensionality same as the number of actions\n",
        "\n",
        "    action_values = np.zeros(environment.nA)\n",
        "    # print(f\"value : {V}\")\n",
        "    for action in range(environment.nA):\n",
        "        # print(f\"action {action}\")\n",
        "        dic_val = environment.env[state][action]\n",
        "        list_1 = [tuple(dic_val.values())]\n",
        "        for probability, next_state, reward, terminated in list_1:\n",
        "            # print(f\"next state {next_state}\")\n",
        "            action_values[action] += probability * \\\n",
        "                (reward + discount_factor * V[int(next_state)])\n",
        "\n",
        "    return action_values\n",
        "\n",
        "\n",
        "# %%\n",
        "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "\n",
        "    # Create a vector of dimensionality as the number of states\n",
        "    # V = np.zeros(environment.nS)\n",
        "    V = [0 for i in range(16)]\n",
        "\n",
        "    for i in range(int(max_iterations)):\n",
        "\n",
        "        # early stopping condition\n",
        "        delta = 0\n",
        "\n",
        "        for state in range(environment.nS):\n",
        "\n",
        "            # Perform the one-step lookahead to get the action values for the state\n",
        "            action_values = one_step_lookahead(\n",
        "                environment, state, V, discount_factor)\n",
        "\n",
        "            # Get the best action value\n",
        "            best_action_value = np.max(action_values)\n",
        "\n",
        "            # Compute the maximum change in the value for each state\n",
        "            delta = max(delta, abs(V[state] - best_action_value))\n",
        "\n",
        "            # Update the best value for the state\n",
        "            V[state] = best_action_value\n",
        "\n",
        "        # Early stopping condition\n",
        "        if(delta < theta):\n",
        "            print('Value iteration converged at iteration #%d' % i)\n",
        "            break\n",
        "\n",
        "    # Find the optimal policy corresponding to the optimal value function\n",
        "    policy = np.zeros((environment.nS, environment.nA))\n",
        "\n",
        "    for state in range(environment.nS):\n",
        "\n",
        "        action_values = one_step_lookahead(\n",
        "            environment, state, V, discount_factor)\n",
        "\n",
        "        # Choose the best action\n",
        "        best_action = np.argmax(action_values)\n",
        "\n",
        "        policy[state][best_action] = 1.0\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "# environment = md.Environment()\n",
        "# dic = environment.env[0][0]\n",
        "# print(f\"dict : {dic}\")\n",
        "# print(tuple(dic.values()))\n",
        "\n",
        "# state = 1\n",
        "# V = [0 for i in range(16)]\n",
        "# discount_factor = 0.5\n",
        "# action_values = one_step_lookahead(environment=environment, state=state,\n",
        "#                                    V=V, discount_factor=discount_factor)\n",
        "\n",
        "# print(f\"Action values : \\n {action_values}\")\n",
        "\n",
        "environment = Environment()\n",
        "discount_factor = 0.8\n",
        "policy, value = value_iteration(\n",
        "    environment=environment, discount_factor=discount_factor)\n",
        "\n",
        "\n",
        "print(f\"policy : \\n{policy}\") \n",
        "print(f\"value : \\n{value}\") \n",
        "\n",
        "\n",
        "# %%\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G23t9C_WMnNM",
        "outputId": "a3fa7f04-f4eb-4837-fa4f-3331530a90e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value iteration converged at iteration #6\n",
            "policy : \n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "value : \n",
            "[-3.6892800000000006, -3.3616000000000006, -2.9520000000000004, -2.4400000000000004, -3.3616000000000006, -2.9520000000000004, -2.4400000000000004, -1.8, -2.9520000000000004, -2.4400000000000004, -1.8, -1.0, -2.4400000000000004, -1.8, -1.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6xvUqsxQMrSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}